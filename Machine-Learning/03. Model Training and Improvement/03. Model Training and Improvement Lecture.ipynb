{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440a923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6836c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b5990c",
   "metadata": {},
   "source": [
    "# 03. Model Training and Improvement\n",
    "### How to train your model\n",
    "* Training and testing set;\n",
    "* Bias-variance tradeoff;\n",
    "* k-fold cross-validation;\n",
    "* Graphical methods: train/test curve, ROC, confusion matrix;\n",
    "* Hyperparameters. Hyperparameter optimization;\n",
    "* Model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16aade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel is working ..\n"
     ]
    }
   ],
   "source": [
    "print('Kernel is working ..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce0dcb9",
   "metadata": {},
   "source": [
    "### IEEE754 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93cec578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2 + 0.1 == 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b170f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10_000_000):\n",
    "    a += 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a472d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99999.99998630969"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d9868",
   "metadata": {},
   "source": [
    "### Scaling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08002d43",
   "metadata": {},
   "source": [
    "For scaling we can use the `MinMaxScaler`. It works like this:\n",
    "$$ x\\prime = \\frac{x - min(x)}{max(x) - min(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13871ec6",
   "metadata": {},
   "source": [
    "For standartization we can use the `StandardScaler`. It works like this:\n",
    "$$ x\\prime =  \\frac{x - \\overline{x}}{s(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca21324",
   "metadata": {},
   "source": [
    "<img src=\"images/lr.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d53f7",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Taming your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fdd41a",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff\n",
    "* When we fit models, we have two main sources of errors\n",
    "    * **Bias** - how far are the predicted from the actual values\n",
    "    * **Variance** - variability of prediction from the actual values\n",
    "* Illustration - shooter aimed at a bullseye target \n",
    "    * High bias - the aim is shifted away from the center\n",
    "    * High variance - the points are more \"spread out\"\n",
    "<img src=\"images/bv.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96293535",
   "metadata": {},
   "source": [
    "`Polynomial wiggle` - when the model explains the data, but between the data it is chaotic. Function in which with a small change in the input data, there is a huge difference in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772624e9",
   "metadata": {},
   "source": [
    "To test if a model is good we need to test it with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3c275",
   "metadata": {},
   "source": [
    "* When we fit several models, they perform differently\n",
    "    * Some are not complex enough (don't describe data well enough)\n",
    "        * **Underfitting** (high bias)\n",
    "    * Some may describe the data \"too well\" and **fail to generalize** when **new** data points are introduced\n",
    "        * **Overfitting** (high variance)\n",
    "* Optimal model: tradeoff between underfitting and overfitting\n",
    "    * Usually, underfitting is easy to spot\n",
    "        * Poor performance w.r.t. some metric\n",
    "    * Overfitting is more complicated\n",
    "        * Many methods exist to prevent overfitting\n",
    "<img src=\"images/ovf.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547188b",
   "metadata": {},
   "source": [
    "`Fitting a model` - Aproximating a data in a certain model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb15483",
   "metadata": {},
   "source": [
    "### Regilarization\n",
    "* Method for finding as good bias-variance tradeoff\n",
    "    * Filter out noise from data\n",
    "    * Handle highly correlated features\n",
    "* **L2** regularization - \"second norm\" (Euclidean): $\\lambda \t\\|\\omega \\| _2^2 \\equiv \\lambda \\displaystyle\\sum_{j=1}^n \\omega^{2}_{j}$\n",
    "    * $ \\lambda $ - regilarization parameter\n",
    "    * Shrinks all model weights by the same value\n",
    "    \n",
    "* **L1** regularization - \"first norm\": $\\lambda \\| \\omega \\| _1 \\equiv \\lambda \\displaystyle\\sum_{j=1}^n \t\\mid \\omega_j \\mid$\n",
    "    * Sets some coefficients to 0: feature selection\n",
    "* In the ideal case, we can use both L1 and L2\n",
    "* Usage: add the regilarization term to the cost function\n",
    "    * $ \\lambda > 0 $, larger $ \\Rightarrow $ stronger regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d08047",
   "metadata": {},
   "source": [
    "$$ J(\\omega) = J(\\tilde{y}, y) + \\lambda\\|\\omega\\| _2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43be8a",
   "metadata": {},
   "source": [
    "### Linear Regression with Regularization\n",
    "* **Ridge** regression - L2\n",
    "    * Cost function: $ J(\\omega) = \\frac{1}{2n} \\displaystyle\\sum^{n}_{i=1}(y_i - \\tilde{y_i})^2 + \\lambda\\|\\omega\\|_2^2 $\n",
    "    * We use the `Ridge` class from scikit liearn, which has parameter \"alpha\" for $ \\lambda $\n",
    "* **LASSO** (**L**east **A**bsolute **S**hrinkage and **S**election **O**perator) - L1\n",
    "    * Cost function: $ J(\\omega) = \\frac{1}{2n} \\displaystyle\\sum^{n}_{i=1}(y_i - \\tilde{y_i})^2 + \\lambda \\|\\omega\\|_1 $\n",
    "    * We use the `Lasso` class from scikit liearn, which has parameter \"alpha\" for $ \\lambda $\n",
    "* **Elastic Net**\n",
    "    * Has both regularization terms\n",
    "    * We use the `ElasticNet` class from scikit liearn, which has parameter \"alpha\" for $ \\lambda $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31526b",
   "metadata": {},
   "source": [
    "We also have regularization in the `LogisticRegression` class, but there we use $C$ instead of $ \\lambda $, $C = \\frac{1}{\\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fce6de",
   "metadata": {},
   "source": [
    "Bigger regularization makes the model care more about the weights than the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308172f",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "Seeing how well your model performs on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a5c2c7",
   "metadata": {},
   "source": [
    "### Training and Testing Sets\n",
    "* One of the most important rules in machine learning is **NEVER test the model with the data you trained it on!**\n",
    "    * The model may \"cheat\" and learn the answers instead of finding structure in the data\n",
    "* Since we usually have one dataset, it's useful to \"hold out\" some of the data for testing\n",
    "    * E.g., **70%** of the data is for training and **30%** - for testing\n",
    "    * We need to take **randomized samples**\n",
    "    * In case of classification, we need stratified samples\n",
    "* scikit-learn has a convinient method for this - `train_test_split` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04272353",
   "metadata": {},
   "source": [
    "If we do a train test split, there is a chance, that the model would know the testing data better than the training one. Why?  Because the \"number\" it produces using the score method is just an extract. If the model knows it's training data way better than the testing one, it has a **high variance**. If the model does not fit it's own data well (the train score is low) it's capacity is not enough, so it has **high bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34cae90",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance\n",
    "* Once we train the model, we use the test data to score it\n",
    "    * Using one of the scoring metrics\n",
    "* Scoring metrics\n",
    "    * **Regression:** usually *coefficient of determination* $R^2$ - proportion of variance predictable from the independent variables\n",
    "        * Other: mean squared error, mean absolute error, explained variance\n",
    "    * **Classification:** usually *accuracy* (how many items have been properly classified)\n",
    "        * OtherL precision, recall, F1\n",
    "* The output from scoring tells us how good the model is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123b97e",
   "metadata": {},
   "source": [
    "### Evaluating Regression\n",
    "* No fixed rules, use your intuition and knowledge about the data\n",
    "* Severa; guidelines\n",
    "    * One metric is usually not enough\n",
    "        * For example, mean squared error and coefficient of determination\n",
    "        * Also useful: mean absolute error and mean squared error\n",
    "    * Create residual plot ($0 - E$: observed minus estimated)\n",
    "        * There should be no visible structure\n",
    "        * If there is some structure in the residuals, the model fails to explain something\n",
    "    * Create a histogram of the residuals\n",
    "        * Most residuals should be \"sufficiently close\" to zero\n",
    "        * There should be no observable structure\n",
    "<img src=\"images/pp.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a6cc5",
   "metadata": {},
   "source": [
    "### Evaluating Classification\n",
    "* **Confusion matrix** (error matrix)\n",
    "    * Shows predicted vs. actual classes\n",
    "    * Simplest case: 2-class classifier\n",
    "        * Can be extended\n",
    "    * FP $\\equiv$ Type I error, FN $\\equiv$ Type II error\n",
    "* Metrics: numbers derived from the confusion matrix\n",
    "    * Accuracy (number of correctly classified samples): $A = \\frac{TP + TN}{TP+TN+FP+FN}$\n",
    "        * If detecting anomalies, accuracy can be misleading\n",
    "    * Precision (how many selected samples are relevant): $P = \\frac{TP}{TP+FP}$\n",
    "    * Recall (how many relevant samples are selected): $R = \\frac{TP}{TP+FN}$\n",
    "    * F1-score: $\\frac{2TP}{2TP + FP + FN} = 2 \\frac{R.P}{R+P}$\n",
    "    * Many more metrics exist (useful for specific cases)\n",
    "\n",
    "<img src=\"images/pn.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7d522",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic\n",
    "* Limited to 2-class classification\n",
    "    * We can use \"1 vs. all\" for more classes\n",
    "* A plot of true positive rate vs. false positive rate\n",
    "    * A \"bisector line\" represents truly random guessing\n",
    "    * Any curve above the line is better than random\n",
    "        * Closer to the upper left corner = better\n",
    "        * Below the line: still better than random, we have to reverse the classifier output\n",
    "<img src=\"images/roc.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b25cda",
   "metadata": {},
   "source": [
    "### Learning and Validation Curves\n",
    "* Plots which allow us to diagnose bias and variance problems\n",
    "    * Some metric (e.g., accuracy) vs. a model parameter (e.g., sample size)\n",
    "    * Plot two curves - for the training and validation data\n",
    "* High bias - accuracu for training and validation is too low\n",
    "    * Solution: add more model features, decrease regularization\n",
    "* High variance - large gap between the two curves\n",
    "    * Solution: remove model features (preprocessing / feature selection / feature engineering, etc.), increase regularization\n",
    "<img src=\"images/lv.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066a3a7",
   "metadata": {},
   "source": [
    "$$ \\tilde{y} = f(X, \\overrightarrow{\\beta}, \\overrightarrow{h}) $$\n",
    "* $ \\overrightarrow{\\beta} $ - parameters for the model\n",
    "* $ \\overrightarrow{h} $ - hyperparameters, eg. ($\\lambda$, $\\alpha$, etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce84c9",
   "metadata": {},
   "source": [
    "If we want to choose the best model, and we have a bunch of them, testing them over a certain test data will show us the model, which performs the most accurate over the test split, not in general. The set, on which we choose the best model and (test once again) is called *validational set* (or development set). `X_val, y_val`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af2220",
   "metadata": {},
   "source": [
    "**We run the `test` set only once!!!**\n",
    "<img src=\"images/tvs.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8950e7",
   "metadata": {},
   "source": [
    "If the data for the validation set is too small, we could make few validation sets and few training sets using one dataset.\n",
    "<img src=\"images/kfc.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c29145",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "* Most algorithms improve their parameters based on the test scores\n",
    "    * This means knowledge of test data may \"leak\" into the algorithm and overfit the data\n",
    "* Solution: cross-validation\n",
    "    * Split all data into $ k $ groups (folds) - usually $ k = 10 $\n",
    "        * **More** samples = **fewer** folds\n",
    "        * Using a KFold splitter\n",
    "    * Each time train with $ k - 1 $ folds and test with other fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ca58e",
   "metadata": {},
   "source": [
    "The more the data is, the less percentage the testing and validation sets could take from our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a071f9",
   "metadata": {},
   "source": [
    "### Improved Technique\n",
    "* Test / train set $\\rightarrow$ faster\n",
    "* Cross-validation $\\rightarrow$ more accurate\n",
    "* Best performance (but even slower): combine the methods\n",
    "    * Leave out some of the data for testing at the beginnng (e.g. 30%)\n",
    "    * Perform cross-validation on the other 70%\n",
    "    * Fine-tune the model and / or select one of many models based on the best cross-validation score\n",
    "    * Run the best model for the other 30%\n",
    "        * We selected the best model based on the training data $\\Rightarrow$ we have some bias\n",
    "            * One model will always have the highest score, even if it's by chance\n",
    "        * This truly out-of-sample method removes (some of) the bias\n",
    "    * **Model selection**: choose the best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213da5b",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "* Techniques for choosing the best model hyperparameters\n",
    "    * Such as regularization\n",
    "* Most widely used: grid search\n",
    "    * \"Brute-force\": specify parameters; run models with all possible parameters combinations; choose the best model\n",
    "* Randomized search - each setting is sampled randomly from a parameter range\n",
    "    * Faster but not guaranteed to produce the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f054a",
   "metadata": {},
   "source": [
    "### Making the Best of Our Models\n",
    "* Usually, we **don't know** right away which algorithm will perform **best**\n",
    "* We *select several algorithm types*\n",
    "    * Fine-tune their parameters using grid search (or some other technique)\n",
    "    * Select the best combination of parameters\n",
    "* After that, we *compare the best algorithms* for each type\n",
    "    * Using the model selection procedure\n",
    "        * Hold-out (test) set + cross-validation set\n",
    "    * Select the best performing model type on the cross-validation set\n",
    "        * Test it om the \"hold-out\" set\n",
    "* Improvements: perform test/train split on the hyperparameter tuning step; use different performance scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a01c06",
   "metadata": {},
   "source": [
    "We could use the `GridSearchCV` class from the `sklearn.model_selection` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3063a5c",
   "metadata": {},
   "source": [
    "## Manipulating Features\n",
    "Making things simpler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7b169",
   "metadata": {},
   "source": [
    "### Guides for Manipulating Features\n",
    "* Main ideas\n",
    "    * **Reduce** the number of features ($\\Rightarrow$ simpler model)\n",
    "    * Keep **relevant** information only\n",
    "* Feature selection\n",
    "    * Removing irrelevant features\n",
    "    * Regularization does a good job at this\n",
    "    * Other methods: dimensionality reduction\n",
    "        * We'll talk about this later in the course\n",
    "* Feature engineering\n",
    "    * Producing new, meaningful features\n",
    "        * Requires a lot of work and domain knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c51d823",
   "metadata": {},
   "source": [
    "Date: 21.09.2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
