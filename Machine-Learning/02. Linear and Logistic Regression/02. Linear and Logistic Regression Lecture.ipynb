{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d87a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "edc7a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d251633",
   "metadata": {},
   "source": [
    "# 02. Linear and Logistic Regression\n",
    "### Simple, yet powerful predictors\n",
    "* Regression – problem statement and motivation;\n",
    "* Ordinary least squares: method, simulated example, implementation on real data;\n",
    "* RANSAC – robust regression model;\n",
    "* Linear regression extensions: polynomial regression, multi-dimensional linear regression;\n",
    "* Classification – problem statement and motivation;\n",
    "* Logistic regression: method, simulated example, real data;\n",
    "* Regularization for regression and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433e15a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel is working..\n"
     ]
    }
   ],
   "source": [
    "print('Kernel is working..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ceb403",
   "metadata": {},
   "source": [
    "Logistic regression has \"Regression\" in it's name, but it does a classification. :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b826af",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Predict continuous values... and torture first-semester students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f644efe",
   "metadata": {},
   "source": [
    "* Regression - predicting continuous variable\n",
    "* Problem statement \n",
    "    * Given pairs of $ (x; y) $ points, create a model\n",
    "        * Input $ x $, output $ y $; **goal: predict $ y $ given $ x $**\n",
    "         * Under the assumption that $ y $ depends linearly on $ x $ (and nothing else)\n",
    "* Modelling function\n",
    "    * $\\tilde{y} = ax + b$\n",
    "    * Many samples: for each sample $ (x_{1}, y_{1}), ..., (x_{n}, y_{n}):$\n",
    "        * $ \\tilde{y_{i}} = ax_{i} + b, i \\in [1;n] $\n",
    "    * Many variables: $ \\tilde{y} = a_{1}x_{1} + a_{2}x_{2} + ... + a_{n}x_{n} + b \\equiv a^{T}X + b $\n",
    "        * Trick: $ a_{0} \\equiv b; x_{0} \\equiv 1 \\Rightarrow \\tilde{y} = a_{0}.1 + a_{1}x_{1} + ... + a_{n}x_{n} = a^{T}x $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f645a",
   "metadata": {},
   "source": [
    "Imagine that we have a matrix. One row is one observation, and one column is a feature (or attribute), so one cell is a variable for an observation. (Assume that the matrix is made out of numbers only). This matrix is what we call the `features`, and we use a `X` sighn for it. Sometimes the underlines show the rank. If there is one line bellow, it's a vector, and if there are two - it's a matrix. By a convention we would say, that the dimensions are n rows by m cols. For a result we would expect a vector `y` containing n rows by 1 col. So we say that we have a process, such as when we give it the data from `X` it producess the results of `y`.\n",
    "\n",
    "$$\n",
    "X_{n,m} = \n",
    "\\begin{pmatrix}\n",
    "a_{1,1} & a_{1,2} & \\cdots & a_{1,m} \\\\\n",
    "a_{2,1} & a_{2,2} & \\cdots & a_{2,m} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "a_{n,1} & a_{n,2} & \\cdots & a_{n,m} \n",
    "\\end{pmatrix} \n",
    "\\Rightarrow f(a_{n}) \\Rightarrow\n",
    "y_{1,n} = \\begin{bmatrix}\n",
    "a_{1} \\\\ a_{2} \\\\ \\vdots \\\\ a_{n} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4926ebf",
   "metadata": {},
   "source": [
    "No system can say for itself whether it is correct or not, someone from outside has to check it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef701634",
   "metadata": {},
   "source": [
    "Looking at the image above, we could see that we have a function $ f(a_{n})$ , well this is basically our model :D This function gives us a result, which we call `prediction`, $ \\tilde{y} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faedbc16",
   "metadata": {},
   "source": [
    "Our job is to make $ \\tilde{y} $ as similar as possible to $ y $ (approximate it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3ca6c",
   "metadata": {},
   "source": [
    "It's good to be able to think about data as points in n-dimentional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070e72c",
   "metadata": {},
   "source": [
    "To measure two points we could use Euclidian distance: $ d(p,q)^{2} = (q_{1} - p_{1})^{2} + (q_{2} - p_{2})^{2} $ or $ d(p,q) = \\sqrt{(q_{1} - p_{1})^{2} + (q_{2} - p_{2})^{2}} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ddb1a2",
   "metadata": {},
   "source": [
    "These \"points\" could tell us how similar are two data observations. But look, there is a difference between $ y_{i} $ and the predicted $ \\tilde{y_{i}} $, so we just subtract them - $ y_{i} - \\tilde{y_{i}}$. Here comes the next question, the result could be negative, that's why we could get the mode $ | y_{i} - \\tilde{y_{i}} |$  or get them as a square root $ (y_{i} - \\tilde{y_{i}})^{2} $. Now that becomes the difference of \"how much is one point far away from the predicted one\".\n",
    "\n",
    "$$ d_{i} = (y_{i} - \\tilde{y_{i}})^{2} $$\n",
    "\n",
    "But we do not care for only one observation, we are interested in all of them, so that's why we can get the sum of all of them, and after that divide it to the mean. It's also called Mean Square Error (MSE) or cost function. We usually use `J` for the MSE.\n",
    "$$ J := \\frac{1}{n} \\sum^{n - 1}_{i = 0} d_{i} \\equiv \\frac{1}{n} \\sum_{i} (y_{i} - \\tilde{y_{i}})^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ee314",
   "metadata": {},
   "source": [
    "Now we have a new goal: find \"the best model\", $ f(a_{n}) $. So let's define what is the best model: well, basically $ min(J) $. And suddenly we have a optimization task :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d10af",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "* Input: $ a, b $; output $ J $\n",
    "* Paraboloid (3D parabola)\n",
    "    * It has exactly one min value\n",
    "        * And we can see it\n",
    "* Intuition\n",
    "    * If the plot was a real object (say, a sheet of some sort), we could slide a ball bearing on it\n",
    "    * After a while, the ball bearing will settle at the \"bottom\" due to gravity\n",
    "    * We can \"simulate\" this: **gradient descent**\n",
    "* Reminder: gradient\n",
    "    * \"Multi-dimensional derivative\"\n",
    "\n",
    "$$ \\nabla J = \n",
    "\\begin{pmatrix} \n",
    "\\frac{\\partial J}{\\partial a} \\\\\n",
    "\\frac{\\partial J}{\\partial b} \n",
    "\\end{pmatrix} $$\n",
    "\n",
    "<img src=\"pics/gradient-descent.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24d6a3",
   "metadata": {},
   "source": [
    "* Iterative algorithm - perform as many times as needed\n",
    "    * Start from some sort of point in the $ (a; b) $ space: $ (a_{0};b_{0}) $\n",
    "    * Decide how big steps to take: number $ a $\n",
    "        * Called **learning rate** in ML terminology\n",
    "    * Use the current $ a, b $ and $ x, y $ to compute $ \\nabla J $\n",
    "        * $ - \\nabla J_{a} $ tells us how much to move in the $ a $ direction in order to get to the minimum\n",
    "        * Similar for $ - \\nabla J_{b} $\n",
    "    * Take a step with size $ a $ in each direction\n",
    "        * $ a_{1} = a_{0} - \\alpha \\nabla J_{a}; b_{1} = b_{0} - \\alpha \\nabla J_{b}$\n",
    "        * $ (a_{1}; b_{1}) $  are the new coordinates\n",
    "    * Repeat the two preceding steps as needed\n",
    "        * Usually, we do this for a fixed number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0b7e9",
   "metadata": {},
   "source": [
    "Imagine that you are in a mountain. You see nothing beside your feet, and your task is to get to the bottom of the mountain. $ \\nabla J $ is the function, that describes the gradient of the mountain. ($a_{0}; b_{0})$ are our starting coordinates. We know how much is the gradient - $ \\nabla J $, so we just go in the reverse direction of it to make a step down. $ \\alpha $ describes our step size. Imagine, that the same way we could be making such a small steps, that we cannot get to the bottom of the mounain by the end of the iterations, the same way we could start going up on the next hill, because our steps are way too big. We call $ \\alpha $ either **\"step size\"** or **\"learning rate\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a551fdf",
   "metadata": {},
   "source": [
    "Well, there is a chance, that we could hit a local minimum, which is higher than the largest minimum. We shouldn't worry about that yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae108ac",
   "metadata": {},
   "source": [
    "So with that said, we do not care about the minimal loss function, but the arguments, that make the minimal loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51423e8a",
   "metadata": {},
   "source": [
    "Getting gradient ascent is as easy: $ a_{1} = a_{0} + \\alpha \\nabla J_{a} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ca6ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = pd.read_fwf(\"https://raw.githubusercontent.com/rupakc/UCI-Data-Analysis/master/Boston%20Housing%20Dataset/Boston%20Housing/housing.data\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4806dac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
       "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
       "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
       "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
       "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
       "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
       "\n",
       "       11    12    13  \n",
       "0  396.90  4.98  24.0  \n",
       "1  396.90  9.14  21.6  \n",
       "2  392.83  4.03  34.7  \n",
       "3  394.63  2.94  33.4  \n",
       "4  396.90  5.33  36.2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5bcba99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     float64\n",
       "1     float64\n",
       "2     float64\n",
       "3       int64\n",
       "4     float64\n",
       "5     float64\n",
       "6     float64\n",
       "7     float64\n",
       "8       int64\n",
       "9     float64\n",
       "10    float64\n",
       "11    float64\n",
       "12    float64\n",
       "13    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b77e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean prices for the houses in K's\n",
    "target = housing_data[13] # y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22d20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = housing_data.drop(columns=[13]) # X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "654e0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06070aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the data into the model\n",
    "linear_regression.fit(attributes, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfc4e178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7198065414937174"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the score of the model\n",
    "linear_regression.score(attributes, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c0090f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = linear_regression.predict(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80be2250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.653807404961373"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(target, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b1307f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      6.109473\n",
       "1      3.528110\n",
       "2     -3.692144\n",
       "3     -4.354644\n",
       "4     -7.716318\n",
       "         ...   \n",
       "501    0.908460\n",
       "502    1.256091\n",
       "503    3.617364\n",
       "504    4.026543\n",
       "505    9.975852\n",
       "Name: 13, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4d0ac46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.653807404961373"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The formula from above\n",
    "((predictions - target) ** 2).sum() / len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffb6823b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.86351800705635"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(((predictions - target) ** 2).sum() / len(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bf83d",
   "metadata": {},
   "source": [
    "This is one pretty goot grade for our model. It makes errors with up to $4.863K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b6902b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(target, bins = \"fd\")\n",
    "\n",
    "plt.xlabel(\"House price [x 1e3 USD]\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e050b73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.532806324110677"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3d23c",
   "metadata": {},
   "source": [
    "As we can see, though, there are some houses, that cost \\\\$5-10K, which is near our MSE. That's pretty bad. We don't want our model to say, that a house for $4.3K costs somewhere around \\\\$0-10K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a7cbc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.15737312486942"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try getting the difference from the original value\n",
    "(np.abs(target - predictions) / target).mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e70a17",
   "metadata": {},
   "source": [
    "So these \\\\$5K are near 17%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b3043",
   "metadata": {},
   "source": [
    "We could check out different parts of our dataset. Like, whats the error for houses between 20 and 25K, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3eb175c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_it_10k = target[target <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ec10622",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_lt_10k = housing_data.loc[targets_it_10k.index].drop(columns = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02e4311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lt_10k = linear_regression.predict(attributes_lt_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7f56568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.878738312612924"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs(targets_it_10k - predictions_lt_10k) / targets_it_10k).mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00807859",
   "metadata": {},
   "source": [
    "So as we expected, \"in small prices we make big mistakes\" - I am not talking about models, btw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40780fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.09281375e-01,  1.49403979e-02,  1.27164577e-02,  3.00565375e+00,\n",
       "       -1.55234852e+01,  4.29955958e+00,  2.84848139e-03, -1.08366345e+00,\n",
       "        1.93258621e-01, -2.42034372e-03, -9.65535221e-01,  9.43510233e-03,\n",
       "       -5.25242783e-01])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36d09d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.30511075009914"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Свободен член (каква щеше да е стойността, ако всички други променливи бяха равни на 0)\n",
    "linear_regression.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f416e2",
   "metadata": {},
   "source": [
    "Each coeficient means how steep is one line, and it depends on $ ax $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "461a82e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>506.0</td>\n",
       "      <td>1.716290</td>\n",
       "      <td>2.653510</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>0.0819</td>\n",
       "      <td>0.250895</td>\n",
       "      <td>2.326717</td>\n",
       "      <td>9.96654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>506.0</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>100.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>506.0</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.46000</td>\n",
       "      <td>5.1900</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>27.74000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>506.0</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>506.0</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.38500</td>\n",
       "      <td>0.4490</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.87100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>506.0</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>3.56100</td>\n",
       "      <td>5.8855</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>8.78000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>506.0</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.90000</td>\n",
       "      <td>45.0250</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>100.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>506.0</td>\n",
       "      <td>3.696228</td>\n",
       "      <td>1.999689</td>\n",
       "      <td>0.58570</td>\n",
       "      <td>2.0737</td>\n",
       "      <td>3.107300</td>\n",
       "      <td>5.112625</td>\n",
       "      <td>9.22290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>506.0</td>\n",
       "      <td>4.332016</td>\n",
       "      <td>1.417166</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>506.0</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>187.00000</td>\n",
       "      <td>279.0000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>711.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>506.0</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>12.60000</td>\n",
       "      <td>17.4000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>22.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>506.0</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>0.32000</td>\n",
       "      <td>375.3775</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>396.90000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>506.0</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>1.73000</td>\n",
       "      <td>6.9500</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>37.97000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>506.0</td>\n",
       "      <td>22.532806</td>\n",
       "      <td>9.197104</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>17.0250</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>50.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count        mean         std        min       25%         50%  \\\n",
       "0   506.0    1.716290    2.653510    0.00632    0.0819    0.250895   \n",
       "1   506.0   11.363636   23.322453    0.00000    0.0000    0.000000   \n",
       "2   506.0   11.136779    6.860353    0.46000    5.1900    9.690000   \n",
       "3   506.0    0.069170    0.253994    0.00000    0.0000    0.000000   \n",
       "4   506.0    0.554695    0.115878    0.38500    0.4490    0.538000   \n",
       "5   506.0    6.284634    0.702617    3.56100    5.8855    6.208500   \n",
       "6   506.0   68.574901   28.148861    2.90000   45.0250   77.500000   \n",
       "7   506.0    3.696228    1.999689    0.58570    2.0737    3.107300   \n",
       "8   506.0    4.332016    1.417166    1.00000    4.0000    4.000000   \n",
       "9   506.0  408.237154  168.537116  187.00000  279.0000  330.000000   \n",
       "10  506.0   18.455534    2.164946   12.60000   17.4000   19.050000   \n",
       "11  506.0  356.674032   91.294864    0.32000  375.3775  391.440000   \n",
       "12  506.0   12.653063    7.141062    1.73000    6.9500   11.360000   \n",
       "13  506.0   22.532806    9.197104    5.00000   17.0250   21.200000   \n",
       "\n",
       "           75%        max  \n",
       "0     2.326717    9.96654  \n",
       "1    12.500000  100.00000  \n",
       "2    18.100000   27.74000  \n",
       "3     0.000000    1.00000  \n",
       "4     0.624000    0.87100  \n",
       "5     6.623500    8.78000  \n",
       "6    94.075000  100.00000  \n",
       "7     5.112625    9.22290  \n",
       "8     5.000000    8.00000  \n",
       "9   666.000000  711.00000  \n",
       "10   20.200000   22.00000  \n",
       "11  396.225000  396.90000  \n",
       "12   16.955000   37.97000  \n",
       "13   25.000000   50.00000  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d5700",
   "metadata": {},
   "source": [
    "Yes, but if you look at the matrix here, we could see, that we need to scale our data, so we could see how they depend. We could scale the data using the `MinMaxScaler` from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fa5d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5de1d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d38a486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 0.000e+00, 4.600e-01, 0.000e+00, 3.850e-01, 3.561e+00,\n",
       "       2.900e+00, 5.857e-01, 1.000e+00, 1.870e+02, 1.260e+01, 3.200e-01,\n",
       "       1.730e+00])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eeae9bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.96022e+00, 1.00000e+02, 2.72800e+01, 1.00000e+00, 4.86000e-01,\n",
       "       5.21900e+00, 9.71000e+01, 8.63720e+00, 7.00000e+00, 5.24000e+02,\n",
       "       9.40000e+00, 3.96580e+02, 3.62400e+01])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.data_range_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d846658c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can see now, the minimum for each column is 0\n",
    "scaler.transform(attributes).min(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec547e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And the max is 1\n",
    "scaler.transform(attributes).max(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9971e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = scaler.fit(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3710c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_with_scaler = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d869d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could intercept the coeficients using the new linear regression\n",
    "#linear_regression_with_scaler.fit(target, attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f5ce2",
   "metadata": {},
   "source": [
    "### How do we work with outliers (out, liers!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d2539",
   "metadata": {},
   "source": [
    "Outliers - anomalies, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167b915",
   "metadata": {},
   "source": [
    "There are a lot of ways of dealing with outliers, one very common of which is RANSAC (RANdom SAmple Consensus) \n",
    "1. Fit the model to a random subsample(\"inliers\")\n",
    "2. Test all data points and include those which are \"near\" the model\n",
    "    * Small enough error, tolerance provided by developer\n",
    "3. Fit the model again\n",
    "4. Estimate the error of the model (difference between first and second)\n",
    "5. Iterate steps 1-4 until performance reaches a threshold number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce583b",
   "metadata": {},
   "source": [
    "Rowbust ML - Machine Learning, that is unsensitive to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca137689",
   "metadata": {},
   "source": [
    "Good thing is, that it's also implemented and ready for us to use ;d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0eabeee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ransac = RANSACRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b31d0519",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = housing_data.drop(columns=[13]) # X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e6cb091d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RANSACRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RANSACRegressor</label><div class=\"sk-toggleable__content\"><pre>RANSACRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RANSACRegressor()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ransac.fit(attributes, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "738f0e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, False, False, False,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True,  True, False,  True, False,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False, False, False,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "        True, False, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True, False, False,  True,  True, False, False,  True,\n",
       "        True, False,  True, False,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True, False,  True, False,\n",
       "        True,  True,  True, False,  True, False,  True,  True, False,\n",
       "        True, False, False, False,  True,  True,  True,  True,  True,\n",
       "        True, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False,  True, False,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "       False, False, False, False, False, False, False,  True,  True,\n",
       "        True, False,  True, False,  True,  True, False,  True,  True,\n",
       "       False, False,  True,  True, False, False, False,  True,  True,\n",
       "       False,  True, False,  True, False,  True,  True, False,  True,\n",
       "        True, False,  True,  True,  True,  True,  True, False, False,\n",
       "       False,  True,  True, False,  True,  True,  True, False, False,\n",
       "        True,  True,  True,  True, False,  True, False,  True,  True,\n",
       "       False, False, False, False, False,  True,  True,  True,  True,\n",
       "        True, False, False,  True, False, False, False,  True,  True,\n",
       "       False, False,  True, False,  True,  True, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False, False, False, False,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True, False,\n",
       "       False,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False, False,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False,  True, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False,  True,  True,  True, False, False, False,  True, False,\n",
       "       False,  True, False, False, False,  True, False,  True, False,\n",
       "        True, False,  True,  True, False, False, False,  True, False,\n",
       "        True,  True, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False,  True,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False,  True,\n",
       "       False,  True,  True, False, False, False, False, False,  True,\n",
       "       False, False,  True, False, False, False,  True, False,  True,\n",
       "       False, False, False, False, False, False, False,  True,  True,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "       False, False,  True, False, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True, False])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ransac.inlier_mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1b66b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5731225296442688"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ransac.inlier_mask_.sum() / len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d9be3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.112320040712635"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs(target - ransac.predict(attributes)) / target).mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43008f3e",
   "metadata": {},
   "source": [
    "D: lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a07c8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers = attributes[ransac.inlier_mask_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a13f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "inlier_targets = target[ransac.inlier_mask_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e1c908c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.89644239, 21.01616743, 20.50194198, 15.33210151, 17.98855403,\n",
       "       19.86479546, 18.13980562, 22.72589902, 18.01392057, 18.08627189,\n",
       "       18.03451278, 20.88122484, 17.54540791, 17.13522491, 13.77960943,\n",
       "       16.5559983 , 17.03506528, 14.43114789, 15.57137432, 15.68752541,\n",
       "       19.46099745, 21.55082581, 15.64286533, 16.0196266 , 15.81570279,\n",
       "       20.84129767, 21.75197631, 21.99126654, 24.00875834, 28.03911914,\n",
       "       31.39267801, 25.47784288, 25.31491533, 21.66017194, 19.9241728 ,\n",
       "       20.85433306, 17.39231155, 12.35070634, 15.85802952, 20.0602964 ,\n",
       "       19.44467072, 25.64312941, 22.27913698, 18.35837367, 21.36538549,\n",
       "       19.16193005, 16.01179048, 16.33985033, 20.39668654, 24.02637286,\n",
       "       25.9405285 , 21.55286976, 21.10496696, 17.85019979, 19.97957779,\n",
       "       23.33211083, 24.16933354, 24.00129245, 22.35173686, 23.31949039,\n",
       "       22.31939207, 21.39248946, 27.07008224, 23.08720926, 24.36792255,\n",
       "       22.44339954, 24.11696862, 25.46168751, 22.44702534, 22.67174416,\n",
       "       26.48650682, 29.17696273, 24.22249111, 24.50730415, 25.06438208,\n",
       "       21.10171621, 28.12854534, 22.56780195, 34.3846938 , 31.35059287,\n",
       "       25.10425202, 26.09914728, 20.58023136, 21.02599181, 18.77693194,\n",
       "       18.97149971, 21.87798223, 22.3509265 , 21.23391977, 20.44170598,\n",
       "       21.00817044, 23.52694843, 21.34426848, 21.26987666, 19.09851676,\n",
       "       18.25754419, 17.98212066, 17.43884805, 16.91043033, 16.22876724,\n",
       "       20.56537484, 14.61964818, 20.3048376 , 17.91772538, 16.30810355,\n",
       "       19.44177617, 18.48016952, 20.76426291, 18.72296875, 10.20871771,\n",
       "       14.38959562, 10.76836795, 15.34685646, 17.32407557, 20.89540046,\n",
       "       17.42778109, 17.60946747, 22.42526965, 23.53252965, 23.98035458,\n",
       "       25.9875155 , 23.78490114, 22.49813468, 18.00003011, 18.76679319,\n",
       "       24.7260601 , 22.42318302, 29.80161426, 24.55081448, 24.15076243,\n",
       "       28.46344477, 30.94972563, 29.93349258, 32.78069467, 28.5663273 ,\n",
       "       30.15518422, 28.74909679, 31.50599486, 32.24170723, 31.48715855,\n",
       "       24.23248412, 23.43203128, 23.09842799, 23.13945555, 19.33450217,\n",
       "       22.05652456, 26.2478433 , 23.55375189, 23.69193034, 20.83607617,\n",
       "       23.05606999, 28.86388182, 23.37199789, 28.54975702, 34.55196084,\n",
       "       29.99912064, 30.46679344, 22.26927593, 31.12003415, 29.66746715,\n",
       "       23.9742352 , 26.17248849, 29.93905728, 26.45713849, 20.4214861 ,\n",
       "       23.48189905, 22.30521178, 26.08679731, 25.35268264, 26.05512634,\n",
       "       27.80930851, 19.50893497, 27.71143632, 32.00908557, 31.34141241,\n",
       "       22.40894691, 30.72712406, 22.22673159, 21.40663104, 25.90647273,\n",
       "       24.85627306, 34.15070087, 30.52450286, 28.61604677, 32.09646761,\n",
       "       30.7436921 , 27.44885804, 30.69587436, 30.38472388, 24.80685586,\n",
       "       23.09311918, 23.02182585, 22.44128704, 27.50691154, 29.11501944,\n",
       "       28.02012668, 23.97378373, 21.06138582, 25.84214711, 26.72517488,\n",
       "       32.25406982, 26.84163322, 29.09007326, 31.95817318, 32.82410835,\n",
       "       27.90513866, 32.11973356, 28.36469858, 23.12443008, 19.91113969,\n",
       "       16.08822529, 22.76242017, 19.53315434, 21.31969741, 22.37327882,\n",
       "       17.22891983, 18.59934263, 18.42190977, 23.06884639, 21.41712034,\n",
       "       23.68099272, 23.11717681, 20.80485303, 16.86153786, 24.19559978,\n",
       "       26.15783336, 23.77115316, 21.27890139, 19.79029969, 22.42107815,\n",
       "       22.43277188, 21.20291699, 19.95010719, 18.5960539 , 22.53685518,\n",
       "       21.27943588, 19.92439872, 25.8635356 , 29.10290182, 18.43357647,\n",
       "       18.32857729, 24.02095611, 25.35478254, 24.77985353, 20.42972286,\n",
       "       32.9534709 , 17.66772668, 15.91869073, 11.9428869 ,  9.75880646,\n",
       "       13.46614644, 14.77347104,  5.40718087,  7.25307355, 11.26854567,\n",
       "       14.04540913, 13.86607976,  3.96487751,  8.99993148,  8.2965527 ,\n",
       "        6.44089605,  7.94291605, 14.67579926, 12.61623747, 19.55238238,\n",
       "       15.93161398, 14.91495478, 17.11536366, 14.38127794, 15.74665669,\n",
       "       16.72767019, 18.98643072, 15.47569248, 15.11086492, 15.32205775,\n",
       "        9.41343169, 18.9487179 , 15.05558151, 18.91406761, 21.7216128 ,\n",
       "       24.182759  , 23.74454374, 17.61499643, 20.14987512, 22.72413204,\n",
       "       19.16542985, 24.43207794, 20.49562043, 24.96462893, 23.74727137])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ransac.predict(inliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8aaf04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.117635814448603"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs(inlier_targets - ransac.predict(inliers)) / inlier_targets).mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01e9ea",
   "metadata": {},
   "source": [
    "Better model, but less data ^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49224b23",
   "metadata": {},
   "source": [
    "We could create new features using polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "46c74860",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c2aea03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PolynomialFeatures()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PolynomialFeatures()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly.fit(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f7860569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 105)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly.transform(attributes).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "900a06ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b0a18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_quadratic = poly.transform(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f4adfb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 6.32000000e-03, 1.80000000e+01, ...,\n",
       "        1.57529610e+05, 1.97656200e+03, 2.48004000e+01],\n",
       "       [1.00000000e+00, 2.73100000e-02, 0.00000000e+00, ...,\n",
       "        1.57529610e+05, 3.62766600e+03, 8.35396000e+01],\n",
       "       [1.00000000e+00, 2.72900000e-02, 0.00000000e+00, ...,\n",
       "        1.54315409e+05, 1.58310490e+03, 1.62409000e+01],\n",
       "       ...,\n",
       "       [1.00000000e+00, 6.07600000e-02, 0.00000000e+00, ...,\n",
       "        1.57529610e+05, 2.23851600e+03, 3.18096000e+01],\n",
       "       [1.00000000e+00, 1.09590000e-01, 0.00000000e+00, ...,\n",
       "        1.54802902e+05, 2.54955600e+03, 4.19904000e+01],\n",
       "       [1.00000000e+00, 4.74100000e-02, 0.00000000e+00, ...,\n",
       "        1.57529610e+05, 3.12757200e+03, 6.20944000e+01]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes_quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "77eb340c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression.fit(attributes_quadratic, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "48fc3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = linear_regression.predict(attributes_quadratic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "07bacd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.494863079869404"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs(target - predictions) / target).mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ec88a4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.26644961e+09, -7.21090869e+00, -1.03135657e+00, -4.75533941e+00,\n",
       "        2.98875593e+01,  4.23817312e+01,  1.13659546e+01,  8.67213370e-01,\n",
       "       -2.62157127e+00, -1.38279509e+00, -1.13741180e-01, -3.10131929e+00,\n",
       "       -5.29020693e-02, -2.35554353e-01, -9.69692295e-02,  5.86938098e-01,\n",
       "        4.15093369e-01,  2.29791165e+00, -1.63608448e+00, -6.44299486e-01,\n",
       "        2.57320357e-02, -6.61480406e-01,  8.24239101e-01, -3.95635290e-03,\n",
       "        4.08158680e-01, -1.62124950e-03, -1.71696311e-01,  8.23328263e-05,\n",
       "        4.89690722e-03, -1.56548999e-01,  2.04304825e-01,  6.09795944e-02,\n",
       "       -1.05887691e-03,  1.80827335e-02, -4.74182120e-03,  1.02176456e-04,\n",
       "       -1.10298056e-03,  1.07903017e-03,  2.70280996e-04,  3.00346724e-02,\n",
       "        5.55953688e-02,  3.42726632e+00,  1.00531970e-01,  2.66928864e-04,\n",
       "        1.74269524e-02,  5.36353823e-02,  1.74517646e-04, -6.34007792e-02,\n",
       "        7.15862281e-03, -3.73316961e-02,  2.98875591e+01, -2.51370629e+01,\n",
       "       -5.99777814e+00, -2.57801575e-02,  1.40376537e+00, -5.68907706e-01,\n",
       "       -5.43174499e-04, -6.48536258e-01,  2.04527964e-02, -4.04773816e-01,\n",
       "       -1.47899786e+01, -1.77876601e+00, -5.19026807e-01,  5.92132870e+00,\n",
       "       -7.32540328e-01, -2.73711708e-02, -4.11252906e+00,  1.06090616e-01,\n",
       "        6.29811457e-01,  1.09831682e+00, -2.87263933e-02, -3.52607489e-01,\n",
       "       -3.49610429e-01, -1.01413443e-02, -3.67697617e-01, -1.14883047e-02,\n",
       "        1.82635543e-03,  3.59942707e-04,  9.35582683e-03,  1.52262218e-03,\n",
       "       -9.07481102e-05,  2.69868194e-03, -1.36771392e-03, -7.32837616e-03,\n",
       "       -7.17509876e-02, -1.32758495e-01,  5.59884877e-03, -1.45609414e-01,\n",
       "        9.31285841e-03, -7.07406340e-02, -1.01414675e-03,  1.23547944e-02,\n",
       "       -1.86836638e-01,  1.12243764e-02,  5.97313235e-03,  1.46989799e-05,\n",
       "        1.00950386e-02, -2.33163949e-04, -1.79958920e-04,  4.85043479e-02,\n",
       "        1.02070979e-02,  1.91171689e-02, -1.22587618e-04, -3.94661356e-04,\n",
       "        2.84924692e-02])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b7e03763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x0', 1266449610.0952754),\n",
       " ('x1', -7.210908687329424),\n",
       " ('x2', -1.0313565681949037),\n",
       " ('x3', -4.7553394084514755),\n",
       " ('x4', 29.887559335035835),\n",
       " ('x5', 42.38173122177334),\n",
       " ('x6', 11.365954564569499),\n",
       " ('x7', 0.8672133702321384),\n",
       " ('x8', -2.621571268455653),\n",
       " ('x9', -1.3827950877805917),\n",
       " ('x10', -0.11374118012951513),\n",
       " ('x11', -3.101319294351478),\n",
       " ('x12', -0.05290206925751347),\n",
       " ('x0^2', -0.23555435306959982),\n",
       " ('x0 x1', -0.09696922947158884),\n",
       " ('x0 x2', 0.5869380977880241),\n",
       " ('x0 x3', 0.4150933688419504),\n",
       " ('x0 x4', 2.2979116489538502),\n",
       " ('x0 x5', -1.6360844788374218),\n",
       " ('x0 x6', -0.6442994858575187),\n",
       " ('x0 x7', 0.02573203565786747),\n",
       " ('x0 x8', -0.6614804057523123),\n",
       " ('x0 x9', 0.8242391007752112),\n",
       " ('x0 x10', -0.003956352903030869),\n",
       " ('x0 x11', 0.4081586797068784),\n",
       " ('x0 x12', -0.0016212494972678293),\n",
       " ('x1^2', -0.171696310908632),\n",
       " ('x1 x2', 8.233282632197358e-05),\n",
       " ('x1 x3', 0.004896907224098551),\n",
       " ('x1 x4', -0.15654899882106188),\n",
       " ('x1 x5', 0.20430482516725484),\n",
       " ('x1 x6', 0.06097959444289891),\n",
       " ('x1 x7', -0.0010588769140906784),\n",
       " ('x1 x8', 0.01808273346773681),\n",
       " ('x1 x9', -0.004741821201621588),\n",
       " ('x1 x10', 0.00010217645572652145),\n",
       " ('x1 x11', -0.0011029805594569009),\n",
       " ('x1 x12', 0.001079030173997353),\n",
       " ('x2^2', 0.00027028099608072953),\n",
       " ('x2 x3', 0.03003467240169322),\n",
       " ('x2 x4', 0.05559536884581813),\n",
       " ('x2 x5', 3.427266315917203),\n",
       " ('x2 x6', 0.10053196959737917),\n",
       " ('x2 x7', 0.00026692886433177243),\n",
       " ('x2 x8', 0.017426952430929787),\n",
       " ('x2 x9', 0.05363538234971831),\n",
       " ('x2 x10', 0.00017451764620624033),\n",
       " ('x2 x11', -0.06340077916090747),\n",
       " ('x2 x12', 0.007158622814367632),\n",
       " ('x3^2', -0.037331696090776245),\n",
       " ('x3 x4', 29.88755911730992),\n",
       " ('x3 x5', -25.13706290507558),\n",
       " ('x3 x6', -5.997778142918396),\n",
       " ('x3 x7', -0.02578015745729776),\n",
       " ('x3 x8', 1.4037653700584054),\n",
       " ('x3 x9', -0.5689077060743959),\n",
       " ('x3 x10', -0.0005431744993822235),\n",
       " ('x3 x11', -0.6485362580760181),\n",
       " ('x3 x12', 0.020452796434144403),\n",
       " ('x4^2', -0.404773816336963),\n",
       " ('x4 x5', -14.78997860671637),\n",
       " ('x4 x6', -1.7787660073087515),\n",
       " ('x4 x7', -0.5190268070516757),\n",
       " ('x4 x8', 5.921328703725057),\n",
       " ('x4 x9', -0.7325403280836157),\n",
       " ('x4 x10', -0.0273711708374475),\n",
       " ('x4 x11', -4.112529058315761),\n",
       " ('x4 x12', 0.10609061625721299),\n",
       " ('x5^2', 0.6298114572474957),\n",
       " ('x5 x6', 1.0983168169088433),\n",
       " ('x5 x7', -0.028726393333994266),\n",
       " ('x5 x8', -0.35260748917103496),\n",
       " ('x5 x9', -0.34961042923474334),\n",
       " ('x5 x10', -0.01014134432020486),\n",
       " ('x5 x11', -0.36769761705254655),\n",
       " ('x5 x12', -0.011488304692274465),\n",
       " ('x6^2', 0.0018263554275490118),\n",
       " ('x6 x7', 0.00035994270659678307),\n",
       " ('x6 x8', 0.009355826829163152),\n",
       " ('x6 x9', 0.0015226221831494136),\n",
       " ('x6 x10', -9.074811019260665e-05),\n",
       " ('x6 x11', 0.0026986819379178906),\n",
       " ('x6 x12', -0.001367713922926228),\n",
       " ('x7^2', -0.007328376159156935),\n",
       " ('x7 x8', -0.07175098758714904),\n",
       " ('x7 x9', -0.13275849516870297),\n",
       " ('x7 x10', 0.0055988487679299315),\n",
       " ('x7 x11', -0.14560941379329229),\n",
       " ('x7 x12', 0.009312858413399017),\n",
       " ('x8^2', -0.07074063400147367),\n",
       " ('x8 x9', -0.0010141467469734142),\n",
       " ('x8 x10', 0.012354794424404791),\n",
       " ('x8 x11', -0.18683663755526944),\n",
       " ('x8 x12', 0.011224376442423356),\n",
       " ('x9^2', 0.005973132350437116),\n",
       " ('x9 x10', 1.46989798892605e-05),\n",
       " ('x9 x11', 0.01009503855911742),\n",
       " ('x9 x12', -0.0002331639494723703),\n",
       " ('x10^2', -0.00017995892003064284),\n",
       " ('x10 x11', 0.04850434788041527),\n",
       " ('x10 x12', 0.010207097894121997),\n",
       " ('x11^2', 0.019117168936238993),\n",
       " ('x11 x12', -0.00012258761809569307),\n",
       " ('x12^2', -0.00039466135583854367)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(poly.get_feature_names_out()[1:], linear_regression.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87f3d7",
   "metadata": {},
   "source": [
    "### Logistic Regression - It makes classification\n",
    "* Classification algorithm (despite its name)\n",
    "* Two classes: negative (0) and positive (1)\n",
    "    * Can be extended to more classes\n",
    "* How does it work?\n",
    "    * Linear regression can give us all kinds of values\n",
    "    * We want to constrain them between 0 and 1\n",
    "    * Approach\n",
    "        * Perform linear regression: $ \\tilde{y} = X_{a} $\n",
    "        * Use the sigmoid function to constran the output $ \\sigma(\\tilde{y}) = \\frac{1}{1 + e^{-\\tilde{y}}} = \\frac{1}{1 + e^{-X_{a}}} $\n",
    "        * Quantization: if $ \\sigma \\geq 0.5 $ return 1, and 0 otherwise\n",
    "            * Remember that we only need to return 0 or 1\n",
    "            * We can also use the raw values as probability measures\n",
    "\n",
    "<img src=\"pics/lr.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b0c430",
   "metadata": {},
   "source": [
    "Date: 14.09.2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
