{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed4624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3fe79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sympy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429507f",
   "metadata": {},
   "source": [
    "# 04. Tree and Ensemble Methods\n",
    "### Making decisionsâ€¦ better\n",
    "* Decision trees; information gain and entropy;\n",
    "* Random forests;\n",
    "* Boosting, AdaBoost;\n",
    "* kNN (k nearest neighbors);\n",
    "* Applications: regression and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6401d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel is working...\n"
     ]
    }
   ],
   "source": [
    "print(\"Kernel is working...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30c2ec",
   "metadata": {},
   "source": [
    "$L_1, L_2 = $ `orig_loss` + $ \\lambda \\| \\omega \\|_2^2, \\omega $ - parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3d420",
   "metadata": {},
   "source": [
    "We hava e linear and a ridge regressions. We score them using MSE on the same data. Which is going to have a higher score? - The Ridge regression always has a lower result. It's job is to generalize the model (Less variance for more bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2ccb6",
   "metadata": {},
   "source": [
    "So in general, when the model \"is more attracted\" to a certain variable he could neglect another one. That's why we use regularization, so it's attraction is splitted equaly above all the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6d55f",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\"To be or not to be...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfcf7df",
   "metadata": {},
   "source": [
    "* Can be used for classification or regression\n",
    "    * **Root**: top node (always a single root)\n",
    "    * **Leaves**: bottom nodes\n",
    "    * Getting an answer: path from root to a leaf\n",
    "* Biggest advantage: easy to interpret\n",
    "* Answer a series of yes / no questions to get the data model\n",
    "    * Similar to the way we decide what to do\n",
    "* We can construct our own decision trees using if-statements\n",
    "    * Machine learning problem: construct the tree without involving \"brain power\"\n",
    "\n",
    "<img src=\"images/dt.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd5958",
   "metadata": {},
   "source": [
    "* Start at the root\n",
    "* At each step decide how to split the data\n",
    "    * Choose the feature (column) that results in the largest **information gain** (IG)\n",
    "* Iterate until every leaf node contains only one class\n",
    "    * To avoid overfitting $\\Rightarrow$ **pruing**(limiting the max depth)\n",
    "* Objective function: maximize IG: $IG(D_p, f) = I(D_p) - \\displaystyle\\sum^m_{j-1} \\frac{N_j}{N_p}I(D_j)$\n",
    "    * $f$ - feature to perform the split on\n",
    "    * $D_p, D_j$ - datasets of the parent and child nodes\n",
    "    * $N_p, N_j$ - number of samples (at parent / child nodes)\n",
    "    * $I$ - **implurity measure**\n",
    "    * More simply, difference between parent and child impurities\n",
    "        * Greater difference = more IG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656f26f",
   "metadata": {},
   "source": [
    "We usually represent the dt's using datasets (split one dataset into two others based on a given question). We aim to make the produced tree as balanced as possible.\n",
    "\n",
    "* Entropy / implurity measure - How much information does a column has about itself. Less entropy equals to more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d1c2b",
   "metadata": {},
   "source": [
    "### Impurity Measures\n",
    "* Most libraries implement binary decision trees\n",
    "    * Each node can have 0, 1 or 2 children\n",
    "    * Reasons: simplicity, reducing the search space\n",
    "* Three common impurity measures\n",
    "    * Entropy - measure of classification uncertainty\n",
    "        * Probability 0 or 1 = no uncertainty\n",
    "        * Probability 0,5 = max uncertainty\n",
    "    * Gini index - similar to entropy\n",
    "        * Criterion to minimize the probability of misclassification\n",
    "        * We usually use one of the measures, as they provide similar results\n",
    "    * Misclassification error\n",
    "        * Linear measure (0 at $p = \\{0; 1\\}$, max at $p = 0.5$)\n",
    "        * **Good for pruning** a tree but worst measure for growing\n",
    "<img src=\"images/dd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71008cdf",
   "metadata": {},
   "source": [
    "\"what we don't know\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb52c02",
   "metadata": {},
   "source": [
    "<img src=\"images/gg.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040afb41",
   "metadata": {},
   "source": [
    "<img src=\"images/tt.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e7a6d",
   "metadata": {},
   "source": [
    "* The left one overfits\n",
    "* They have different complexity\n",
    "    * Left: $O(N)$\n",
    "    * Right: $O( \\log_{2}N)$\n",
    "* We can work with trees using the `DecisionTreeClassifier` class from `scikit-learn`.\n",
    "* Trees do not have effect from scaling the data.\n",
    "* There are models like bruning for normalization of the model.\n",
    "* Orcham's razor - Check it out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d341112",
   "metadata": {},
   "source": [
    "## Decision Forests\n",
    "It's even harder to decide..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64be0e8",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "* Combinations (ensembles) of decision trees\n",
    "* Idea: combine many weak learners (models that perform slightly better than random)\n",
    "    * Draw a bootstrap sample (random with replacement) of size $n$\n",
    "    * Grow $k$ decision trees on the bootstrap sample\n",
    "        * At each node, **randomly select $d$ features** and split based on max IG\n",
    "    * Aggregate the prediction by majority vote\n",
    "* Differences with decision trees\n",
    "    * Forests use a random subset of features (trees use all features)\n",
    "    * A little harder to interpret than decision trees :(\n",
    "* Advantages :)))\n",
    "    * Better (lower) generalization error\n",
    "    * Less susceptible to overfitting\n",
    "    * Less hyperparameter tuning (in practice, we usually care about $k$ only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1c07c",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "* Short for \"**Ada**prive **Boost**ing\"\n",
    "    * Another method to combine weak learners into a strong one\n",
    "* Algorithm\n",
    "    * Train a weak learner on a random subset (without replacement) of the test data\n",
    "    * Draw another random subset and add 50% of the previously misclassified samples; train another weak learner on that\n",
    "    * Find the training samples on which both learners disagree to train a third weak learner\n",
    "    * Combine the three weak learners via majority voting\n",
    "* These algorithms tend to overfit the data\n",
    "    * We have to check variance carefully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70836f90",
   "metadata": {},
   "source": [
    "Other cool algorithms:\n",
    "* XGBoost - Overfits a lot\n",
    "* LightBGM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e0edf",
   "metadata": {},
   "source": [
    "So we have 3 types of ensembling - `Bagging`, `Boosting` and `Stacking`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec9cd7",
   "metadata": {},
   "source": [
    "Date: 28.09.2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
